{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11443290,"sourceType":"datasetVersion","datasetId":7168666}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Define dataset path\ndataset_path = \"/kaggle/input/dataset/inaturalist_12K\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:21:37.151853Z","iopub.execute_input":"2025-04-19T00:21:37.152183Z","iopub.status.idle":"2025-04-19T00:21:37.155619Z","shell.execute_reply.started":"2025-04-19T00:21:37.152165Z","shell.execute_reply":"2025-04-19T00:21:37.154877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"5fb34431b405eb21dc0f263e5b3cf2c15fdc7471\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nimport wandb\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\n# Define dataset path\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndataset_path = \"/kaggle/input/dataset/inaturalist_12K\"\n\ndef get_dataloaders(dataset_path, augment, batch_size=32):\n    # Define transforms\n    transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ]) if augment else transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ])\n\n    full_dataset = ImageFolder(root=f\"{dataset_path}/train\", transform=transform)\n\n    # ✅ Class-balanced 80/20 split\n    \n    targets = np.array(full_dataset.targets)\n    strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    for train_idx, val_idx in strat_split.split(np.zeros(len(targets)), targets):\n        train_subset = torch.utils.data.Subset(full_dataset, train_idx)\n        val_subset = torch.utils.data.Subset(full_dataset, val_idx)\n\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader\n\ntrain_loader, val_loader = get_dataloaders(dataset_path, augment=True, batch_size=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:21:38.099798Z","iopub.execute_input":"2025-04-19T00:21:38.100295Z","iopub.status.idle":"2025-04-19T00:21:56.363877Z","shell.execute_reply.started":"2025-04-19T00:21:38.100273Z","shell.execute_reply":"2025-04-19T00:21:56.363000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\n\nclass CNN_Model(nn.Module):\n    def __init__(self, \n                 filter_sizes,              # list of conv filters\n                 dense_neurons,             # neurons in dense layer\n                 activation=\"relu\",         # activation function\n                 dropout=0.0,               # dropout rate\n                 batch_norm=False,          # whether to use BatchNorm\n                 input_shape=(3, 224, 224), # image shape\n                 num_classes=10):           # number of output classes\n        super(CNN_Model, self).__init__()\n\n        # Activation function class (module, not functional)\n        act_layer = {\n            \"relu\": nn.ReLU,\n            \"gelu\": nn.GELU,\n            \"silu\": nn.SiLU,\n            \"mish\": nn.Mish\n        }[activation.lower()]\n        \n        in_channels = input_shape[0]\n        layers = []\n\n        # Convolution blocks\n        for i, out_channels in enumerate(filter_sizes):\n            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))  # padding=1 to preserve size\n            if batch_norm:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(act_layer())\n            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))  # downsample by 2\n            if dropout > 0:\n                layers.append(nn.Dropout2d(dropout))\n            in_channels = out_channels\n\n        self.conv_blocks = nn.Sequential(*layers)\n\n        # Automatically infer flatten size\n        with torch.no_grad():\n            dummy = torch.zeros(1, *input_shape)\n            flatten_dim = self.conv_blocks(dummy).view(1, -1).shape[1]\n\n        self.classifier = nn.Sequential(\n            nn.Linear(flatten_dim, dense_neurons),\n            act_layer(),\n            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n            nn.Linear(dense_neurons, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.classifier(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:21:56.365183Z","iopub.execute_input":"2025-04-19T00:21:56.365570Z","iopub.status.idle":"2025-04-19T00:21:56.374073Z","shell.execute_reply.started":"2025-04-19T00:21:56.365550Z","shell.execute_reply":"2025-04-19T00:21:56.373381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Initialize WandB (resumes existing run if needed)\nwandb.init(project=\"Assignment_2_CNN\", resume=True)\ndef train_cnn_model(model, train_loader, val_loader, config, device):\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"]\n    )\n\n    best_val_acc = 0.0\n\n    for epoch in range(config[\"epochs\"]):\n        model.train()\n        total_loss, correct, total = 0.0, 0, 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        train_acc = 100 * correct / total\n        train_loss = total_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, preds = torch.max(outputs, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_acc = 100 * val_correct / val_total\n        val_loss /= len(val_loader)\n\n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n        # Logging to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n\n        print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n\n    wandb.run.summary[\"best_val_accuracy\"] = best_val_acc\n    return model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:22:00.521170Z","iopub.execute_input":"2025-04-19T00:22:00.521814Z","iopub.status.idle":"2025-04-19T00:22:10.526195Z","shell.execute_reply.started":"2025-04-19T00:22:00.521781Z","shell.execute_reply":"2025-04-19T00:22:10.525500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nsweep_config = {\n    \"method\": \"bayes\",\n    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"epochs\": {\"values\": [10,15,20]},\n        \"filter_sizes\": {\n            \"values\": [\n                [16, 32, 32, 32, 256],\n                [128, 128, 128, 128, 128],\n                [32, 64, 64, 128, 128],\n                [32, 64, 128, 128, 256],\n            ]\n        },\n        \"dense_neurons\": {\"values\": [128, 256, 512]},\n        \"activation\": {\"values\": [\"gelu\",\"relu\", \"silu\", \"mish\"]},\n        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n        \"optimizer\": {\"values\": [\"adam\"]},\n        \"dropout\": {\"values\": [0.2, 0.3]},\n        \"data_augmentation\": {\"values\": [True, False]},\n        \"batch_norm\": {\"values\": [True, False]},\n        \"weight_decay\": {\"values\": [0, 0.0005]},\n    }\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:46:59.717093Z","iopub.execute_input":"2025-04-18T17:46:59.717323Z","iopub.status.idle":"2025-04-18T17:46:59.723049Z","shell.execute_reply.started":"2025-04-18T17:46:59.717306Z","shell.execute_reply":"2025-04-18T17:46:59.722497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train_wandb():\n    with wandb.init():\n        config = wandb.config\n\n        # ✅ Load dataset\n        dataset_path = \"/kaggle/input/dataset/inaturalist_12K\"\n        train_loader, val_loader = get_dataloaders(\n            dataset_path=dataset_path,\n            augment=config.data_augmentation,\n            batch_size=getattr(config, \"batch_size\", 32)\n        )\n\n        # ✅ Build model from config\n        model = CNN_Model(\n            filter_sizes=config.filter_sizes,\n     \n            \n            dense_neurons=config.dense_neurons,\n            activation=config.activation,\n            dropout=config.dropout,\n            batch_norm=config.batch_norm,\n            input_shape=(3, 224, 224),\n            num_classes=10  \n        ).to(device)\n\n        # ✅ Optional: log model internals\n        wandb.watch(model, log=\"all\", log_freq=100)\n\n        # ✅ Train with early stopping\n        train_cnn_model(model, train_loader, val_loader, config, device)\n\n# Launch sweep\nsweep_id = wandb.sweep(sweep_config, project=\"Assignment_2_CNN\")\nwandb.agent(sweep_id, function=train_wandb, count=15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:47:01.810944Z","iopub.execute_input":"2025-04-18T17:47:01.811652Z","iopub.status.idle":"2025-04-18T22:16:58.314049Z","shell.execute_reply.started":"2025-04-18T17:47:01.811617Z","shell.execute_reply":"2025-04-18T22:16:58.313435Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing With Saved model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport numpy as np\nimport wandb  # Add this\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndataset_path = \"/kaggle/input/dataset/inaturalist_12K\"\n\n# Transformation: Resize, Convert to Tensor, and Normalize\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load the test dataset\ntest_dataset = ImageFolder(root=f\"{dataset_path}/val\", transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n\n# Get class names\nclass_names = test_dataset.classes\nprint(f\"Test samples: {len(test_dataset)}\")\n\n# Initialize the model\nmodel = CNN_Model(\n    filter_sizes=[128, 128, 128, 128, 128],  \n    dense_neurons=256,  \n    activation=\"mish\", \n    dropout=0.2, \n    batch_norm=True,  \n    input_shape=(3, 224, 224),\n    num_classes=10  \n).to(device)\n\n# Load the best model weights\nmodel.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\nmodel.eval()\n\n# Evaluation on test data\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\ntest_accuracy = 100 * correct / total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\n\nwandb.init(project=\"Assignment_2_CNN\", resume=True)\n# ✅ Log accuracy to wandb\nwandb.log({\"Test Accuracy\": test_accuracy})\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:47:30.449246Z","iopub.execute_input":"2025-04-19T00:47:30.449567Z","iopub.status.idle":"2025-04-19T00:48:18.028465Z","shell.execute_reply.started":"2025-04-19T00:47:30.449543Z","shell.execute_reply":"2025-04-19T00:48:18.027874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport wandb\nimport torch\n\n# Collect 30 test samples and their predictions\nmodel.eval()\nimages_shown = 0\nimages_to_show = 30\nimages_list, preds_list, labels_list = [], [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        for i in range(images.size(0)):\n            if images_shown >= images_to_show:\n                break\n            images_list.append(images[i].cpu())\n            preds_list.append(predicted[i].cpu())\n            labels_list.append(labels[i].cpu())\n            images_shown += 1\n\n        if images_shown >= images_to_show:\n            break\n\n# Denormalize images\nmean = torch.tensor([0.485, 0.456, 0.406])\nstd = torch.tensor([0.229, 0.224, 0.225])\nimages_list = [(img.permute(1, 2, 0) * std + mean).numpy() for img in images_list]\n\n# Plot 10×3 grid with color-coded titles\nfig, axes = plt.subplots(10, 3, figsize=(10, 30))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(np.clip(images_list[i], 0, 1))\n    true_label = class_names[labels_list[i]]\n    pred_label = class_names[preds_list[i]]\n\n    title_color = \"black\" if true_label == pred_label else \"red\"\n    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=8, color=title_color)\n    ax.axis(\"off\")\n\nplt.tight_layout()\n\n# Log to wandb\n# wandb.log({\"Test Predictions Grid\": wandb.Image(fig)})\n# Save the figure first\nfig.savefig(\"test_predictions_grid.png\")\n\n# Then log it from file\nwandb.log({\"Test Predictions Grid\": wandb.Image(\"test_predictions_grid.png\")})\n\n\n\n# Show the figure in notebook\nplt.show()\nplt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T00:48:25.301447Z","iopub.execute_input":"2025-04-19T00:48:25.301717Z","iopub.status.idle":"2025-04-19T00:48:31.251748Z","shell.execute_reply.started":"2025-04-19T00:48:25.301696Z","shell.execute_reply":"2025-04-19T00:48:31.250686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}